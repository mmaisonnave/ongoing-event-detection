{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "Maisonnave et al 2020 - Event Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GfGYcTcjYYgX",
        "tZ_oCAKNWyUd",
        "yBF_FQzsWyU8",
        "f_DuF3dwWyVF",
        "wTJQUwr6WyVP",
        "3RsJ1OnzWyVY",
        "xBTFnUsdWyVe",
        "-SMUPv2GWyVm",
        "MGjzxzefWyVs",
        "MWHeN-ewWyVy",
        "Xo84ChaQWyV4",
        "sMDIz6b7WyWC",
        "wNWmZn7oYfVc",
        "Cw5ZKzB8Wut6",
        "oJwKT2ruWuuQ",
        "Z_MSJmKoWuub",
        "OQSbK9emWuuk",
        "30chMCFZWuut",
        "Gusg6-NFWuu1",
        "6ffziguyWuu6",
        "kaWOzlI0WuvA",
        "iogSb9T5WuvH",
        "HfFTTo9sWuvM",
        "Tz6gDoiHWuvU",
        "DmxUAI5GWuvZ"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5nEA984WyUa"
      },
      "source": [
        "# Ongoing Event Detection Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tis8LliY1es4"
      },
      "source": [
        "**Contributors:** [Mariano Maisonnave](https://cs.uns.edu.ar/~mmaisonnave), [Fernando Delbianco](https://scholar.google.com.ar/citations?user=cQ3vD7oAAAAJ&hl=en&oi=ao), [Fernando Tohmé](https://scholar.google.com.ar/citations?user=butwPD4AAAAJ&hl=en&oi=ao), [Ana Maguitman](https://scholar.google.com.ar/citations?user=upxByNEAAAAJ&hl=en&oi=ao) and [Evangelos Milios](https://scholar.google.com.ar/citations?user=ME8aQywAAAAJ&hl=en&oi=ao)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w2IZd2Qk1Q-"
      },
      "source": [
        "This notebook contains the complete code used for the experiments conducted for the paper: \"[Improving Event Detection using Contextual Word and Sentence Embeddings](https://arxiv.org/abs/2007.01379)\".\n",
        "\n",
        "Although this is the complete code for that article, that work was not run on Google Colab. Mainly because of storage and computational limitations. If you run into trouble running this in Google Colab, we recommend to download the notebook and run it locally once all the requirements are met, and all the required data is downloaded. \n",
        "\n",
        "The notebook was used for the development and testing of the models. The final models were run by copying all the code for each model into one single file (baseline_svm.py, baseline_cnn.py, proposed_model.py), and then ran from the command line following the syntaxis described below.  \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "***Additional files for running the experiments can be found at [https://github.com/mmaisonnave/ongoing-event-detection](https://github.com/mmaisonnave/ongoing-event-detection)***\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**How to run the proposed model (RNN):**\n",
        "```\n",
        "python3 proposed_model.py --seed=1 --model_name=prop_model_seed1 --epochs=3000 --bert_enabled=True --pos_enabled=False --dep_enabled=False --tag_enabled=False --words_enabled=False --entities_enabled=False --spacyvecs_enabled=False --bert_sent_enabled=False --lstm_layers=7 --evaluate_only=False\n",
        "```\n",
        "**How to run the state-of-the-art baseline model (CNN):**\n",
        "```\n",
        "python3 baseline_cnn.py --seed=1 --model_name=baseline_seed1 --words_enabled=False --entities_enabled=True --positions_enabled=True --windows_size=11 --epochs=3000 --batch_size=50 --evaluate_only=False --patience=200\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "Parameter description:\n",
        "  - seed: Used to obtain reproducible results.\n",
        "  - model_name: The name of the model. Used to store weights, architecture and results on a folder with the same name as the model. \n",
        "  - epochs: Max number of epochs the model will train. If high, the model will stop because of early stopping before that for the number of epochs.\n",
        "  - evaluate_only: If \"True\" the model will not train.\n",
        "  - patience: Patience value for the early stopping (number of epochs without improvement before stop training. \n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "Parameters specific to the CNN model\n",
        "  - words_enabled: If True, the Word2vec feature will be used in the model.\n",
        "  - entities_enabled: If True, the Entity embedding feature will be used in the model.\n",
        "  - positions_enabled: If True, the Position embedding feature will be used in the model.\n",
        "  - windows_size: Size of the context window. An even number is required. For example, if windows_size=5 for predicting each token, two previous and two posterior tokens will be used for prediction. \n",
        "  - batch_size: Batch size.\n",
        "Parameters specific to the RNN model\n",
        "  - bert_enabled: If True, the BERT embedding feature will be used in the model.\n",
        "  - pos_enabled: If True, the Part-Of-Speech (simplified) embedding feature will be used in the model.\n",
        "  - dep_enabled: If True, the Dependency Parser Tag embedding feature will be used in the model.\n",
        "  - tag_enabled: If True, the Part-Of-Speech (detailed) embedding feature will be used in the model.\n",
        "  - words_enabled: If True, the Word2vec feature will be used in the model.\n",
        "  - entities_enabled: If True, the Entity embedding feature will be used in the model.\n",
        "  - spacyvecs_enabled: if True, the Spacy Contex-sensitive embedding feature will be used in the model.\n",
        "  - bert_sent_enabled: If True, the sentence BERT embedding feature will be used in the model. This embedding is computed as adding all the individual BERT embeddings.\n",
        "  - lstm_layers: A comma-separated list of numbers. This list represents the number of hidden units of each layer. For example, lstm_layers=7 builds a single-layer Bi-LSTM model with seven hidden units. A lstm_layer=15,5 builds a two-layer Bi-LSTM model with 15 and 5 hidden units, in the first and second layer, respectively.\n",
        "```\n",
        "\n",
        "**How to run the classical baseline model (SVM):**\n",
        "```\n",
        "python3 baseline_svm.py [-h] [--kernel KERNEL] [--balanced] [--standarize] \n",
        "\n",
        "Parameters specific to the SVM model\n",
        "  -h, --help            show this help message and exit\n",
        "  -k KERNEL, --kernel KERNEL\n",
        "                        Specifies the kernel type to be used in the algorithm.\n",
        "                        It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’.\n",
        "                        If none is given, ‘rbf’ will be used.\n",
        "  -b, --balanced        The “balanced” mode uses the values of y to\n",
        "                        automatically adjust weights inversely proportional to\n",
        "                        class frequencies in the input data as n_samples /\n",
        "                        (n_classes * np.bincount(y)).\n",
        "  -s, --standarize      Specifies wheter to standarize the data or not.\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Required software and data:**\n",
        "*   Software: [spaCy](https://spacy.io/), [Keras](https://keras.io/), [TensorFlow](https://www.tensorflow.org/), [NumPy](https://numpy.org/), [Gensim](https://radimrehurek.com/gensim/).\n",
        "*   Data: [Event Detection Dataset](https://cs.uns.edu.ar/~mmaisonnave/resources/ED_data), [Word2Vec](https://code.google.com/archive/p/word2vec/) \n",
        "\n",
        "\n",
        "The complete list of python packages used and their version can be found [here](https://cs.uns.edu.ar/~mmaisonnave/resources/ED_data/requirements.txt). This requirments were obtained using `pip freeze`.\n",
        "\n",
        "\n",
        "\n",
        "Note that the code uses BERT pre-trained word embeddings. We built this embedding by summing the last four layers of the BERT pre-trained model. This process resulted in a 768-dimension word embedding for each word. We computed these embeddings with another notebook, and we add the embeddings to the data. Therefore, the process for building those embeddings is not described here. If you are interested in that script, you can email the first author of the work.\n",
        "\n",
        "\n",
        "***If you plan to use or replicate our work and run into any issue, do not hesitate to contact the first author of the work.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqwyvptKzzcd"
      },
      "source": [
        "**Disclaimer**\n",
        "\n",
        "The following notebook is for having a grasp on how the model was built and executed. Data download and configuration is required. For example, the Word2vec word embeddings have to be downloaded, as well as the event detection dataset ([link](https://cs.uns.edu.ar/~mmaisonnave/resources/ED_data)). \n",
        " \n",
        "*If you plan to replicate the models, we strongly suggest you contact the first author of this work.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "519H2J3IALIc"
      },
      "source": [
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Ongoing Event Detection Tool</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"https://cs.uns.edu.ar/~mmaisonnave/resources/ED_code/\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">Mariano Maisonnave</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>.<br />Based on a work at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://arxiv.org/abs/2007.01379\" rel=\"dct:source\">https://arxiv.org/abs/2007.01379</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgMjG9k2HAGC"
      },
      "source": [
        "## Classical Baseline (SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kgrygdU60b1"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M3-82X2HDx8",
        "outputId": "984b8742-4836-49aa-e85b-535d9a27166e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "import datetime\n",
        "def info(msg):\n",
        "    print(f'{datetime.datetime.now()} [ INFO  ] {msg}')\n",
        "def warning(msg):\n",
        "    print(f'{datetime.datetime.now()} [WARNING] {msg}')\n",
        "def ok(msg):\n",
        "    print(f'{datetime.datetime.now()} [  OK   ] {msg}')\n",
        "\n",
        "info('Loading spacy library')\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-15 19:42:19.823343 [ INFO  ] Loading spacy library\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDI-5SB1Syki"
      },
      "source": [
        "### arguments and parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNkaFyTVSaG5"
      },
      "source": [
        "**If running from console this cell should not be executed.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8nacUxHHsF0"
      },
      "source": [
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument(\"-k\", \"--kernel\", help=\"Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’. If none is given, ‘rbf’ will be used.\",\n",
        "                    type=str)\n",
        "\n",
        "parser.add_argument(\"-b\", \"--balanced\", help=\"The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\",\n",
        "                    action=\"store_true\")\n",
        "\n",
        "parser.add_argument(\"-s\", \"--standarize\", help=\"Specifies wheter to standarize the data or not.\",\n",
        "                    action=\"store_true\")\n",
        "\n",
        "\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "info(f\"STANDARIZE option {'ENABLED' if args.standarize else 'DISABLE'}\")\n",
        "info(f\"BALANCED   option {'ENABLED' if args.balanced else 'DISABLED'}.\")\n",
        "kernel = 'rbf'\n",
        "if args.kernel:\n",
        "    kernel = args.kernel\n",
        "else:\n",
        "    warning('Kernel not provieded, rbf assumed.')\n",
        "info(f\"{kernel.upper()} kernel selected.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vC-_6TH5e8r"
      },
      "source": [
        "**For running the following cell you should download the required data from https://cs.uns.edu.ar/~mmaisonnave/resources/ED_data/**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhFGnpaOHxAT"
      },
      "source": [
        "data_training_path = 'data/training'\n",
        "data_testing_path = 'data/testing'\n",
        "assert os.path.exists(data_training_path) and os.path.exists(data_testing_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsuOiOT6Hy4m"
      },
      "source": [
        "info('Searching training and testing files')\n",
        "training_files = []\n",
        "for dirpath, _, filenames in os.walk(data_training_path):\n",
        "    for filename in filenames:\n",
        "        file_ = os.path.join(dirpath,filename)\n",
        "        assert file_.endswith('xml')\n",
        "        training_files.append(file_)\n",
        "info(f'Cantidad de archivos de entrenamiento (*.xml): {len(training_files)}')\n",
        "\n",
        "testing_files = []\n",
        "for dirpath, _, filenames in os.walk(data_testing_path):\n",
        "    for filename in filenames:\n",
        "        file_ = os.path.join(dirpath,filename)\n",
        "        assert file_.endswith('xml')\n",
        "        testing_files.append(file_)\n",
        "info(f'Cantidad de archivos de testing (*.xml): {len(testing_files)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDwVCZojH0ML"
      },
      "source": [
        "# my_word2vec is a dictionary that give the vector for each word, I built this \n",
        "# from the Word2vec you download from here: https://code.google.com/archive/p/word2vec/\n",
        "# I reduced the file to have only the words that are present in the dataset used \n",
        "# here. If you need this file you can email me. If you prefer, you can also \n",
        "# build yourself the original file you can download in the previous url.\n",
        "my_word2vec = pickle.load(open('my_word2vec.p','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm98hLeXTq5h"
      },
      "source": [
        "**Auxiliary methods from reading the files from the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvRjtfXQH2Ex"
      },
      "source": [
        "def _read_file(file_):\n",
        "    content = re.findall('<sentence>(.*)</sentence>', open(file_, 'r').read(), re.DOTALL)[0].strip()\n",
        "    texto = re.sub('</*event>','',content)\n",
        "    \n",
        "    match_obj = re.search('<event>.*?</event>', content)\n",
        "    events = []\n",
        "    while not match_obj is None:\n",
        "        idx = match_obj.start()\n",
        "        trigger = match_obj.group()[7:-8] #deleting <event></event> tags\n",
        "        events.append(((idx,idx+len(trigger)),trigger))\n",
        "        content = re.sub('<event>(.*?)</event>','\\g<1>',content, count=1)\n",
        "        match_obj = re.search('<event>.*?</event>', content)\n",
        "        \n",
        "    assert all([texto[ini:fin]==trigger for ((ini,fin),trigger) in events])\n",
        "    return texto, events\n",
        "\n",
        "def get_matrices(file_):\n",
        "    texto, events = _read_file(file_)\n",
        "    indices = set([ini for ((ini,fin),trigger) in events])\n",
        "    doc = nlp(texto)\n",
        "    \n",
        "    y = np.array([1 if token.idx in indices else 0 for token in doc])\n",
        "    assert np.sum(y)==len(events)\n",
        "    X = np.zeros(shape=(len(doc), 300))\n",
        "    \n",
        "    for idx,token in enumerate(doc):\n",
        "        if token.text in my_word2vec:\n",
        "            X[idx,:] = my_word2vec[token.text]\n",
        "#         else:\n",
        "#             print(token.text)\n",
        "#         if token.has_vector:\n",
        "#             X[idx,:] = token.vector\n",
        "            \n",
        "    return X,y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0IAR7FpTvrH"
      },
      "source": [
        "### Reading training files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tANwBBYyINjH"
      },
      "source": [
        "info('Reading training files')\n",
        "Xs = []\n",
        "Ys = []\n",
        "for file_ in training_files:\n",
        "    X, y = get_matrices(file_)\n",
        "    Xs.append(X)\n",
        "    Ys.append(y)\n",
        "    \n",
        "X_train = np.vstack(Xs)\n",
        "y_train = np.hstack(Ys)\n",
        "info(X_train.shape)\n",
        "info(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1H706YKTzE1"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mceOr_X9IOyu"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "info(f'Training SVM classifier con {kernel}')\n",
        "if args.standarize:\n",
        "    clf = make_pipeline(\n",
        "        StandardScaler(), \n",
        "        SVC(gamma='auto', kernel=kernel, class_weight='balanced' if args.balanced else None)\n",
        "    )\n",
        "else:\n",
        "    clf = SVC(gamma='scale', kernel=kernel, class_weight='balanced' if args.balanced else None)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHZK8CzdT2O8"
      },
      "source": [
        "### Reading testing files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InC0TFhSIQJ8"
      },
      "source": [
        "info('Reading testing files')\n",
        "\n",
        "Xs = []\n",
        "Ys = []\n",
        "for file_ in testing_files:\n",
        "    X, y = get_matrices(file_)\n",
        "    Xs.append(X)\n",
        "    Ys.append(y)\n",
        "    \n",
        "X_test = np.vstack(Xs)\n",
        "y_test = np.hstack(Ys)\n",
        "info(X_test.shape)\n",
        "info(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpVss4ymT_lR"
      },
      "source": [
        "### Computing training metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c3NJMt2IRMG"
      },
      "source": [
        "info('Computing Training Metrics:')\n",
        "yhat = clf.predict(X_train)\n",
        "true_positives = np.sum(yhat[y_train==1]==y_train[y_train==1])\n",
        "true_negatives =  np.sum(yhat[y_train==0]==y_train[y_train==0])\n",
        "\n",
        "false_positives = np.sum(yhat[y_train==1]!=y_train[y_train==1])\n",
        "false_negatives = np.sum(yhat[y_train==0]!=y_train[y_train==0])\n",
        "\n",
        "total_positives = np.sum(y_train==1)\n",
        "total_negatives = np.sum(y_train==0)\n",
        "\n",
        "sens = true_positives/total_positives\n",
        "spec = true_negatives/total_negatives\n",
        "f1 = 2*((sens*spec)/(sens+spec))\n",
        "acc = clf.score(X_train, y_train)\n",
        "\n",
        "\n",
        "info(f'\\t - accuracy:    {acc:4.3f}')\n",
        "info(f'\\t - sensitivity: {sens:4.3f}')\n",
        "info(f'\\t - specificity: {sens:4.3f}')\n",
        "info(f'\\t - f1-score:    {sens:4.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rqqyPQBUDqM"
      },
      "source": [
        "### Computing testing metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCUIkhxtISh7"
      },
      "source": [
        "info('Computing Testing Metrics:')\n",
        "\n",
        "yhat = clf.predict(X_test)\n",
        "true_positives = np.sum(yhat[y_test==1]==y_test[y_test==1])\n",
        "true_negatives =  np.sum(yhat[y_test==0]==y_test[y_test==0])\n",
        "\n",
        "false_positives = np.sum(yhat[y_test==1]!=y_test[y_test==1])\n",
        "false_negatives = np.sum(yhat[y_test==0]!=y_test[y_test==0])\n",
        "\n",
        "\n",
        "\n",
        "total_positives = np.sum(y_test==1)\n",
        "total_negatives = np.sum(y_test==0)\n",
        "\n",
        "sens = true_positives/total_positives\n",
        "spec = true_negatives/total_negatives\n",
        "f1 = 2*((sens*spec)/(sens+spec))\n",
        "acc = clf.score(X_test, y_test)\n",
        "\n",
        "\n",
        "info(f'\\t - accuracy:    {acc:4.3f}')\n",
        "info(f'\\t - sensitivity: {sens:4.3f}')\n",
        "info(f'\\t - specificity: {sens:4.3f}')\n",
        "info(f'\\t - f1-score:    {sens:4.3f}')\n",
        "\n",
        "print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfGYcTcjYYgX"
      },
      "source": [
        "## State-of-the-art Baseline (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ_oCAKNWyUd"
      },
      "source": [
        "### arguments and parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtKSYOLvV4uc"
      },
      "source": [
        "import sys\n",
        "\n",
        "my_seed = 1\n",
        "windows_size = 3\n",
        "model_name = 'default'\n",
        "\n",
        "patience = 200\n",
        "\n",
        "nro_of_filters = 150 # fixed according to Nguyen et al. 2015\n",
        "epochs = 1\n",
        "batch_size = 50\n",
        "\n",
        "entities_enabled = True\n",
        "positions_enabled = True\n",
        "words_enabled = True\n",
        "evaluate_only=False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxAWaT41VHo7"
      },
      "source": [
        "**If running from console this cell should not be executed.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baCbUP1kWyUi"
      },
      "source": [
        "import  getopt\n",
        "\n",
        "\n",
        "# \n",
        "\n",
        "argv = sys.argv[1:]\n",
        "\n",
        "try:\n",
        "    opts, args = getopt.getopt(argv,\"\",[\"seed=\",\n",
        "                                        \"model_name=\",\n",
        "                                        \"words_enabled=\",\n",
        "                                        \"entities_enabled=\",\n",
        "                                        \"positions_enabled=\",\n",
        "                                        \"windows_size=\",\n",
        "                                        \"epochs=\",\n",
        "                                        \"batch_size=\",\n",
        "                                        'evaluate_only=',\n",
        "                                        'patience='    \n",
        "                                       ])\n",
        "except getopt.GetoptError:\n",
        "    print('Bad parameters')\n",
        "    sys.exit(2)\n",
        "for opt, arg in opts:\n",
        "    if opt == \"--seed\":\n",
        "        my_seed = int(arg)\n",
        "    elif opt == \"--model_name\":\n",
        "        model_name = arg\n",
        "    elif opt == \"--words_enabled\":\n",
        "        words_enabled = arg==\"True\"\n",
        "    elif opt == \"--entities_enabled\":\n",
        "        entities_enabled = arg==\"True\"\n",
        "    elif opt == \"--positions_enabled\":\n",
        "        positions_enabled = arg==\"True\"\n",
        "    elif opt == \"--windows_size\":\n",
        "        windows_size = int(arg)\n",
        "    elif opt == \"--epochs\":\n",
        "        epochs = int(arg)\n",
        "    elif opt == \"--batch_size\":\n",
        "        batch_size = int(arg)\n",
        "    elif opt == '--evaluate_only':\n",
        "        evaluate_only = arg=='True'\n",
        "    elif opt == '--patience':\n",
        "        patience = int(arg)\n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwMV3djiWyUz",
        "outputId": "7c907286-6e54-4ea2-a2d7-626f00d82537"
      },
      "source": [
        "model_name = '{}_({})_w{}'.format(model_name,my_seed,windows_size)\n",
        "                  \n",
        "def info(cad):\n",
        "    print('[ INFO  ] {}'.format(cad))\n",
        "def ok(cad):\n",
        "    print('[  OK   ] {}'.format(cad))\n",
        "def warning(cad):\n",
        "    print('[WARNING] {}'.format(cad))\n",
        "    \n",
        "\n",
        "info('seed:              {}'.format(my_seed))\n",
        "info('model_name:        {}'.format(model_name))\n",
        "info('entities_enabled:  {}'.format(entities_enabled))\n",
        "info('words_enabled:     {}'.format(words_enabled))\n",
        "info('positions_enabled: {}'.format(positions_enabled))\n",
        "info('windows size =     {}'.format(windows_size))\n",
        "info('no. epochs =       {}'.format(epochs))\n",
        "if evaluate_only:\n",
        "    warning('EVALUATE ONLY.')\n",
        "if epochs<=10:\n",
        "    warning('POCAS EPOCHS.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ INFO  ] seed:              1\n",
            "[ INFO  ] model_name:        default_(1)_w3\n",
            "[ INFO  ] entities_enabled:  False\n",
            "[ INFO  ] words_enabled:     True\n",
            "[ INFO  ] positions_enabled: True\n",
            "[ INFO  ] windows size =     3\n",
            "[ INFO  ] no. epochs =       1\n",
            "[WARNING] POCAS EPOCHS.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBF_FQzsWyU8"
      },
      "source": [
        "### Paths "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoFJOIZNWyU-"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(my_seed)\n",
        "\n",
        "# For saving the model information:\n",
        "home_path = '/home/mariano/work/python3.workspace/Event Detection - Experimentos Finales/Nguyen Replication/'\n",
        "\n",
        "# Where to look up for the event detection data set (it is not in the same format as in the webpage)\n",
        "data_path = '/home/mariano/work/python3.workspace/Event Detection - Experimentos Finales/data/'\n",
        "\n",
        "# if not os.path.exists(home_path):\n",
        "#     home_path = '/home/maiso/Event Detection/Nguyen Replication/'\n",
        "#     data_path = '/home/maiso/Event Detection/data/'\n",
        "\n",
        "\n",
        "word2vec_path = os.path.join(data_path,'word2vec/GoogleNews-vectors-negative300.bin')\n",
        "my_word2vec_path = os.path.join(data_path, 'word2vec/my_word2vec.p')\n",
        "\n",
        "training_sents_path = os.path.join(data_path,'training_sents.p') \n",
        "testing_sents_path = os.path.join(data_path,'testing_sents.p')\n",
        "\n",
        "\n",
        "\n",
        "os.makedirs(os.path.join(home_path,'models/{}'.format(model_name)), exist_ok=True)\n",
        "\n",
        "best_model_weights = os.path.join(home_path,'models/{}/weights.h5'.format(model_name))\n",
        "best_model_architecture = os.path.join(home_path,'models/{}/architecture.json'.format(model_name))\n",
        "history_path = os.path.join(home_path, 'models/{}/history.p'.format(model_name))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_DuF3dwWyVF"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDBw0aFuWyVG",
        "outputId": "e3c35909-8096-4dbc-a444-a44a683295cb"
      },
      "source": [
        "import pickle\n",
        "oraciones = pickle.load(open(training_sents_path, 'rb'))\n",
        "oraciones_testing = pickle.load(open(testing_sents_path, 'rb'))\n",
        "info('Oraciones training/validation retrieved (size = {:4.0f}).'.format(len(oraciones)))\n",
        "info('Oraciones testing             retrieved (size = {:4.0f}).'.format(len(oraciones_testing)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ INFO  ] Oraciones training/validation retrieved (size = 2000).\n",
            "[ INFO  ] Oraciones testing             retrieved (size =  200).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsiC6MwhWyVO"
      },
      "source": [
        "### Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTJQUwr6WyVP"
      },
      "source": [
        "##### positions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpZ62m5nWyVR"
      },
      "source": [
        "mitad = int(windows_size/2)\n",
        "def token2position(token_idx, oracion):\n",
        "    return list(map(abs,list(range(-mitad,mitad+1))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RsJ1OnzWyVY"
      },
      "source": [
        "##### entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urMPx0imWyVY",
        "outputId": "0ddd3342-59e2-4226-e1c9-df1d265a8af2"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# # # # # # # # # # # # # # \n",
        "# GENERACIÓN ENTITY VOCAB #\n",
        "# # # # # # # # # # # # # # \n",
        "entity_vocab = set()\n",
        "for oracion in oraciones:\n",
        "    if not 'spacy_doc' in oracion:\n",
        "        oracion['spacy_doc'] = nlp(oracion['texto'])\n",
        "    assert all([token.idx==i and (token.idx+len(token.text))==f for (i,f),token in zip(oracion['tokens'], oracion['spacy_doc'])]), '{}!={}'.format(token.i,i)\n",
        "    e = [token.ent_iob_+'-'+token.ent_type_ for token in oracion['spacy_doc']]\n",
        "    entity_vocab.update(set(e))\n",
        "\n",
        "for oracion in oraciones_testing:\n",
        "    if not 'spacy_doc' in oracion:\n",
        "        oracion['spacy_doc'] = nlp(oracion['texto'])\n",
        "    assert all([token.idx==i and (token.idx+len(token.text))==f for (i,f),token in zip(oracion['tokens'], oracion['spacy_doc'])]), '{}!={}'.format(token.i,i)\n",
        "    e = [token.ent_iob_+'-'+token.ent_type_ for token in oracion['spacy_doc']]\n",
        "    entity_vocab.update(set(e))\n",
        "\n",
        "entity_vocab.add('[PAD]')\n",
        "entity_vocab = list(entity_vocab)\n",
        "ent2index = dict([(ent,index) for index,ent in enumerate(entity_vocab)])\n",
        "info('Entity Vocabulary Size: {}'.format(len(ent2index)))\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # \n",
        "# DEFINICIÓN FUNCIÓN PARA OBTENER FEATURES LIST #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # \n",
        "\n",
        "def token2entity(token_idx, oracion):\n",
        "    doc = oracion['spacy_doc']\n",
        "    token = doc[token_idx]\n",
        "    l = int(windows_size/2)\n",
        "    idx = token.i\n",
        "    ini = max(0,idx-l)\n",
        "    fin = min(len(doc),1+idx+l)\n",
        "    \n",
        "    x = [ent2index[t.ent_iob_+'-'+t.ent_type_] for t in doc[ini:fin]]\n",
        "\n",
        "    if (ini>idx-l):\n",
        "        # PAD al ppio\n",
        "        dif = ini -(idx-l)\n",
        "        x = [ent2index['[PAD]']]*dif +x\n",
        "    if (fin< 1+idx+l):\n",
        "        # PAD al final\n",
        "        dif = 1+ idx+l-fin\n",
        "        x = x+  [ent2index['[PAD]']]*dif\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ INFO  ] Entity Vocabulary Size: 35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBTFnUsdWyVe"
      },
      "source": [
        "##### word2vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSaWpM70WyVg",
        "outputId": "f1cc5614-f14d-4bd8-d37d-438e3445d125"
      },
      "source": [
        "import gensim\n",
        "# # # # # # # # # # # # \n",
        "# GENERACIÓN WORD2VEC #\n",
        "# # # # # # # # # # # # \n",
        "if not os.path.isfile(my_word2vec_path):\n",
        "    word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True) \n",
        "    # GENERACIÓN\n",
        "    my_word2vec = {}\n",
        "    for oracion in oraciones:\n",
        "        for ini,fin in oracion['tokens']:\n",
        "            token = oracion['texto'][ini:fin]\n",
        "            if token in word2vec_model:\n",
        "                my_word2vec[token] = word2vec_model[token]\n",
        "    for oracion in oraciones_testing:\n",
        "        for ini,fin in oracion['tokens']:\n",
        "            token = oracion['texto'][ini:fin]\n",
        "            if token in word2vec_model:\n",
        "                my_word2vec[token] = word2vec_model[token]\n",
        "                \n",
        "    pickle.dump(my_word2vec, open(my_word2vec_path, 'wb'))\n",
        "else:\n",
        "    my_word2vec = pickle.load(open(my_word2vec_path,'rb'))\n",
        "    \n",
        "vocab = set(my_word2vec.keys())\n",
        "vocab.update(set(['[UNK]','[PAD]']))\n",
        "vocab = list(vocab)\n",
        "info('Word Vocabulary Size: {}'.format(len(vocab)))\n",
        "word2index = dict([(word,idx) for idx, word in enumerate(vocab)])\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # \n",
        "# DEFINICIÓN FUNCIÓN PARA OBTENER FEATURES LIST #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # \n",
        "\n",
        "def token2word(token_idx, oracion):\n",
        "    token_list = oracion['tokens']\n",
        "    texto = oracion['texto']\n",
        "    idx = token_idx\n",
        "    \n",
        "    l = int(windows_size/2)\n",
        "    ini = max(0,idx-l)\n",
        "    fin = min(len(token_list),1+idx+l)\n",
        "\n",
        "    x = [word2index[texto[i:j]] if texto[i:j] in word2index else word2index['[UNK]'] for i,j in token_list[ini:fin]]\n",
        "    if (ini>idx-l):\n",
        "        # PAD al ppio\n",
        "        dif = ini -(idx-l)\n",
        "        x = [word2index['[PAD]']]*dif +x\n",
        "    if (fin< 1+idx+l):\n",
        "        # PAD al final\n",
        "        dif = 1+ idx+l-fin\n",
        "        x = x+  [word2index['[PAD]']]*dif\n",
        "\n",
        "    return x\n",
        "\n",
        "# token2word(0, oraciones[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ INFO  ] Word Vocabulary Size: 8647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qdwBqDAWyVl"
      },
      "source": [
        "### The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SMUPv2GWyVm"
      },
      "source": [
        "##### Custom metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13wq0T9KWyVn",
        "outputId": "23f4d061-2dba-4c12-c265-30ab0412c072"
      },
      "source": [
        "import warnings\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
        "    from keras.callbacks import K\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    sens =  true_positives / (possible_positives + K.epsilon())\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
        "    spec = true_negatives / (possible_negatives + K.epsilon())\n",
        "    return 2*((sens*spec)/(sens+spec+K.epsilon()))\n",
        "\n",
        "def sensitivity(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "def specificity(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
        "    return true_negatives / (possible_negatives + K.epsilon())\n",
        "info('f1-score, sensitivity and specifity custom metrics defined.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ INFO  ] f1-score, sensitivity and specifity custom metrics defined.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGjzxzefWyVs"
      },
      "source": [
        "##### callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7q1yl3RWyVu",
        "outputId": "bd14b21a-cb96-4651-fe99-9aaa20fa2395"
      },
      "source": [
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "if not evaluate_only:\n",
        "    mc = ModelCheckpoint(best_model_weights, monitor='val_f1_score',save_weights_only=True, save_best_only=True,mode='max')\n",
        "    es = EarlyStopping(monitor='val_f1_score', mode='max', verbose=1, patience=patience)\n",
        "info('Early Sopping Defined (patience={})'.format(patience))\n",
        "info('Saving model weights into: {}'.format(best_model_architecture))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ INFO  ] Early Sopping Defined (patience=200)\n",
            "[ INFO  ] Saving model weights into: /home/mariano/work/python3.workspace/Event Detection - Experimentos Finales/Nguyen Replication/models/default_(1)_w3/architecture.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWHeN-ewWyVy"
      },
      "source": [
        "##### the architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2Kzi8WkWyV0",
        "outputId": "8d80a49b-efb7-43d5-f31e-85a5b258367f"
      },
      "source": [
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Embedding,Input, Dropout# Create the model\n",
        "import numpy as np\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Lambda\n",
        "import tensorflow as tf\n",
        "from keras.backend import expand_dims\n",
        "from keras.layers.merge import concatenate\n",
        "import tensorflow\n",
        "from keras.models import model_from_json\n",
        "# clear session\n",
        "from keras.regularizers import l1_l2\n",
        "\n",
        "# tensorflow.keras.backend.clear_session()\n",
        "# Parametros|\n",
        "\n",
        "def define_architecture():\n",
        "    embedding_size = 300\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "\n",
        "\n",
        "    # Modelo\n",
        "    words_input_layer = Input((None,))\n",
        "    entities_input_layer = Input((None,))\n",
        "    positions_input_layer = Input((None,))\n",
        "\n",
        "    # transfer learning with word2vec\n",
        "    emb_matrix = np.random.random((len(vocab),300))\n",
        "    for idx,word in enumerate(vocab):\n",
        "        if word in my_word2vec:\n",
        "            emb_matrix[idx,:] = my_word2vec[word]\n",
        "\n",
        "    words_emb_layer = Embedding(vocab_size, 300, input_length=windows_size, weights=[emb_matrix],activity_regularizer=l1_l2(0.01,0.01),)(words_input_layer)\n",
        "\n",
        "    entities_emb_layer = Embedding(len(entity_vocab), 50, input_length=windows_size,activity_regularizer=l1_l2(0.01,0.01),)(entities_input_layer)\n",
        "    positions_emb_layer = Embedding(int(windows_size/2)+1, 50, input_length=windows_size,activity_regularizer=l1_l2(0.01,0.01),)(positions_input_layer)\n",
        "    \n",
        "    vec_length = 0\n",
        "    embedding_layers = []\n",
        "    if words_enabled:\n",
        "        embedding_layers.append(words_emb_layer)\n",
        "        vec_length+=300\n",
        "    if entities_enabled:\n",
        "        embedding_layers.append(entities_emb_layer)\n",
        "        vec_length+=50\n",
        "    if positions_enabled:\n",
        "        embedding_layers.append(positions_emb_layer)\n",
        "        vec_length+=50\n",
        "\n",
        "    if len(embedding_layers)>1:\n",
        "        concat_layer  = concatenate(embedding_layers)\n",
        "    else:\n",
        "        concat_layer = embedding_layers[0] # la lista tiene un solo elemento lo recuperamos y lo usamos como entrada\n",
        "\n",
        "    lamb_layer = Lambda(lambda x: expand_dims(x, 3))(concat_layer)\n",
        "    # conv_layer = Conv2D(nro_of_filters, (size_of_filters, windows_size),activity_regularizer=l1_l2(0.01,0.01),  padding='same', activation='relu', kernel_constraint=maxnorm(3))(lamb_layer)\n",
        "\n",
        "    pool_layers = []\n",
        "    \n",
        "    if windows_size==1:\n",
        "        conv_layer_1 = Conv2D(nro_of_filters, (1, vec_length),  padding='valid', activation='relu', kernel_constraint=maxnorm(3))(lamb_layer)\n",
        "        pool_layer_1 = MaxPooling2D(pool_size=(1,1))(conv_layer_1)\n",
        "        pool_layers.append(pool_layer_1)\n",
        "    if windows_size>=2:\n",
        "        conv_layer_2 = Conv2D(nro_of_filters, ( 2, vec_length),  padding='valid', activation='relu', kernel_constraint=maxnorm(3))(lamb_layer)\n",
        "        pool_layer_2 = MaxPooling2D(pool_size=(windows_size-1,1))(conv_layer_2)\n",
        "        pool_layers.append(pool_layer_2)\n",
        "    \n",
        "    if windows_size>=3:\n",
        "        conv_layer_3 = Conv2D(nro_of_filters, ( 3, vec_length),  padding='valid', activation='relu', kernel_constraint=maxnorm(3))(lamb_layer)\n",
        "        pool_layer_3 = MaxPooling2D(pool_size=(windows_size-2,1))(conv_layer_3)\n",
        "        pool_layers.append(pool_layer_3)\n",
        "\n",
        "    if windows_size>=4:\n",
        "        conv_layer_4 = Conv2D(nro_of_filters, ( 4, vec_length),  padding='valid', activation='relu', kernel_constraint=maxnorm(3))(lamb_layer)\n",
        "        pool_layer_4 = MaxPooling2D(pool_size=(windows_size-3,1))(conv_layer_4)\n",
        "        pool_layers.append(pool_layer_4)\n",
        "\n",
        "    if windows_size>=5:\n",
        "        conv_layer_5 = Conv2D(nro_of_filters, ( 5, vec_length),  padding='valid', activation='relu', kernel_constraint=maxnorm(3))(lamb_layer)\n",
        "        pool_layer_5 = MaxPooling2D(pool_size=(windows_size-4,1))(conv_layer_5)\n",
        "        pool_layers.append(pool_layer_5)\n",
        "\n",
        "\n",
        "    if len(pool_layers)==1:\n",
        "        conv_layer = pool_layers[0]\n",
        "    else:\n",
        "        conv_layer = concatenate(pool_layers, axis=1)\n",
        "\n",
        "    #pool_layer = MaxPooling2D(pool_size=(10,1))(conv_layer)\n",
        "    flat_layer = Flatten()(conv_layer)\n",
        "    dropout_layer = Dropout(0.5)(flat_layer)\n",
        "    dense_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
        "\n",
        "\n",
        "    inputs = []\n",
        "    if words_enabled:\n",
        "        inputs.append(words_input_layer)\n",
        "    if entities_enabled:\n",
        "        inputs.append(entities_input_layer)\n",
        "    if positions_enabled:\n",
        "        inputs.append(positions_input_layer)\n",
        "    model = Model(inputs=inputs, outputs=[dense_layer])\n",
        "    model_json = model.to_json()\n",
        "    with open(best_model_architecture, \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "        \n",
        "    return model\n",
        "\n",
        "        \n",
        "        \n",
        "if evaluate_only:\n",
        "    try:\n",
        "        # load json and create model\n",
        "        print('[WARNING] Model retrieved from JSON')\n",
        "        json_file = open(best_model_architecture, 'r')\n",
        "        loaded_model_json = json_file.read()\n",
        "        json_file.close()\n",
        "        model = model_from_json(loaded_model_json)\n",
        "    except:\n",
        "        print('[WARNING] JSON could not be properly loaded')\n",
        "        model = define_architecture()\n",
        "else:\n",
        "    model = define_architecture()\n",
        "    \n",
        "\n",
        "# Compilación\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy',sensitivity,specificity, f1_score])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 3, 300)       2594100     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 3, 50)        100         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 3, 350)       0           embedding_1[0][0]                \n",
            "                                                                 embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 3, 350, 1)    0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 2, 1, 150)    105150      lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 1, 1, 150)    157650      lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 150)    0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 150)    0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 2, 1, 150)    0           max_pooling2d_1[0][0]            \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 300)          0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 300)          0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            301         dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,857,301\n",
            "Trainable params: 2,857,301\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo84ChaQWyV4"
      },
      "source": [
        "##### the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELpVBlHwWyV7",
        "outputId": "b61f6822-372c-4131-d3cb-31e108a7dcf3"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "x1 = np.array([token2word(idx, oracion) for oracion in oraciones for idx,_ in enumerate(oracion['tokens'])])\n",
        "x2 = np.array([token2entity(idx, oracion) for oracion in oraciones for idx,_ in enumerate(oracion['tokens'])])\n",
        "x3 = np.array([token2position(idx, oracion) for oracion in oraciones for idx,_ in enumerate(oracion['tokens'])])\n",
        "\n",
        "\n",
        "y = np.array([valor for oracion in oraciones for valor in oracion['etiquetas']])\n",
        "\n",
        "\n",
        "indices = np.arange(len(x1))\n",
        "np.random.shuffle(indices)\n",
        "train_idx,test_idx = indices[:int(len(indices)*.80)], indices[int(len(indices)*.8):]\n",
        "\n",
        "x1_train, x1_test = x1[train_idx], x1[test_idx]\n",
        "x2_train, x2_test = x2[train_idx], x2[test_idx]\n",
        "x3_train, x3_test = x3[train_idx], x3[test_idx]\n",
        "\n",
        "y_train, y_val = y[train_idx],y[test_idx]\n",
        "\n",
        "flags = [words_enabled, entities_enabled, positions_enabled]\n",
        "info('Training set size: {:6,.0f} tokens'.format(len(y_train)))\n",
        "info('Testing  set size: {:6,.0f} tokens'.format(len(y_val)))\n",
        "info('Word token list for first training data:     {}'.format(x1_train[0]))\n",
        "info('Entity token list for first training data:   {}'.format(x2_train[0]))\n",
        "info('Position token list for first training data: {}'.format(x3_train[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ INFO  ] Training set size: 61,303 tokens\n",
            "[ INFO  ] Testing  set size: 15,326 tokens\n",
            "[ INFO  ] Word token list for first training data:     [1937 6860 3455]\n",
            "[ INFO  ] Entity token list for first training data:   [11 11 11]\n",
            "[ INFO  ] Position token list for first training data: [1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMDIz6b7WyWC"
      },
      "source": [
        "##### the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "8Qv5SdLsWyWD",
        "outputId": "5b0b9a41-f69f-49f3-9943-3d46604cb038"
      },
      "source": [
        "train_data = []\n",
        "val_data = []\n",
        "if words_enabled:\n",
        "    train_data.append(np.vstack(x1_train))\n",
        "    val_data.append(np.vstack(x1_test))\n",
        "if entities_enabled:\n",
        "    train_data.append(np.vstack(x2_train))\n",
        "    val_data.append(np.vstack(x2_test))\n",
        "if positions_enabled:\n",
        "    train_data.append(np.vstack(x3_train))\n",
        "    val_data.append(np.vstack(x3_test))\n",
        "    \n",
        "if not evaluate_only:\n",
        "    info('Starting training.')\n",
        "    history = model.fit(train_data,\n",
        "                        np.vstack(y_train),\n",
        "                        batch_size=batch_size,\n",
        "                        callbacks=[mc, es],\n",
        "                        validation_data=(val_data, np.vstack(y_val)),\n",
        "                        epochs=epochs)\n",
        "    \n",
        "    info('Finish training, saving weights.')\n",
        "    pickle.dump(history.history, open(history_path, 'wb'))\n",
        "else:    \n",
        "    warning('NO TRAINING, EVALUATE ONLY')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ INFO  ] Starting training.\n",
            "Train on 61303 samples, validate on 15326 samples\n",
            "Epoch 1/1\n",
            "61303/61303 [==============================] - 76s 1ms/step - loss: 44.1457 - acc: 0.9372 - sensitivity: 0.1568 - specificity: 0.9918 - f1_score: 0.2152 - val_loss: 10.3884 - val_acc: 0.9314 - val_sensitivity: 0.1520 - val_specificity: 0.9905 - val_f1_score: 0.2091\n",
            "[ INFO  ] Finish training, saving weights.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXc619IMWyWL",
        "outputId": "222f41c6-78f5-47e3-dcba-fe5ce7a3072d"
      },
      "source": [
        "print('[  OK   ] # # # # # # # # # ON BATCH # # # # # # # # # # # ')\n",
        "print('[  OK   ] Loading best weights.')\n",
        "model.load_weights(best_model_weights)\n",
        "\n",
        "print('[  OK   ] Loading test set')\n",
        "x1 = np.array([token2word(idx, oracion) for oracion in oraciones_testing for idx,_ in enumerate(oracion['tokens'])])\n",
        "x2 = np.array([token2entity(idx, oracion) for oracion in oraciones_testing for idx,_ in enumerate(oracion['tokens'])])\n",
        "x3 = np.array([token2position(idx, oracion) for oracion in oraciones_testing for idx,_ in enumerate(oracion['tokens'])])\n",
        "\n",
        "y = np.array([valor for oracion in oraciones_testing for valor in oracion['etiquetas']])\n",
        "\n",
        "test_data = []\n",
        "if words_enabled:\n",
        "    test_data.append(np.vstack(x1))\n",
        "if entities_enabled:\n",
        "    test_data.append(np.vstack(x2))\n",
        "if positions_enabled:\n",
        "    test_data.append(np.vstack(x3))\n",
        "\n",
        "\n",
        "print('[  OK   ] Performing the prediction.')\n",
        "print('[ INFO  ] TRAINING')\n",
        "values = model.evaluate(train_data,\n",
        "                    np.vstack(np.hstack([y_train])),\n",
        "                        batch_size=batch_size\n",
        "    )\n",
        "for metric,value in zip(model.metrics_names, values):\n",
        "    print('{}: {:5.4f}'.format(metric, value))\n",
        "\n",
        "print('[ INFO  ] VAL')\n",
        "values = model.evaluate(val_data,\n",
        "                    np.vstack(np.hstack([y_val])),\n",
        "                        batch_size=batch_size\n",
        "    )\n",
        "for metric,value in zip(model.metrics_names, values):\n",
        "    print('{}: {:5.4f}'.format(metric, value))\n",
        "\n",
        "\n",
        "print('[ INFO  ] TESTING')\n",
        "values = model.evaluate(test_data,\n",
        "                    np.vstack(y),\n",
        "                        batch_size=batch_size\n",
        "    )\n",
        "for metric,value in zip(model.metrics_names, values):\n",
        "    print('{}: {:5.4f}'.format(metric, value))\n",
        "\n",
        "\n",
        "     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  OK   ] # # # # # # # # # ON BATCH # # # # # # # # # # # \n",
            "[  OK   ] Loading best weights.\n",
            "[  OK   ] Loading test set\n",
            "[  OK   ] Performing the prediction.\n",
            "[ INFO  ] TRAINING\n",
            "61303/61303 [==============================] - 4s 69us/step\n",
            "loss: 9.7533\n",
            "acc: 0.9415\n",
            "sensitivity: 0.1866\n",
            "specificity: 0.9942\n",
            "f1_score: 0.2521\n",
            "[ INFO  ] VAL\n",
            "15326/15326 [==============================] - 1s 69us/step\n",
            "loss: 10.3884\n",
            "acc: 0.9314\n",
            "sensitivity: 0.1520\n",
            "specificity: 0.9905\n",
            "f1_score: 0.2091\n",
            "[ INFO  ] TESTING\n",
            "7382/7382 [==============================] - 1s 68us/step\n",
            "loss: 12.1221\n",
            "acc: 0.9455\n",
            "sensitivity: 0.1860\n",
            "specificity: 0.9898\n",
            "f1_score: 0.2511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNWmZn7oYfVc"
      },
      "source": [
        "## Proposed Model (RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw5ZKzB8Wut6"
      },
      "source": [
        "### arguments and parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB7fTI98WSzg"
      },
      "source": [
        "import sys\n",
        "import sys, getopt\n",
        "\n",
        "my_seed = 1\n",
        "model_name = 'default'\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "bert_enabled = True\n",
        "bert_sent_enabled=True\n",
        "dep_enabled = False\n",
        "tag_enabled = False\n",
        "pos_enabled = False\n",
        "words_enabled = False\n",
        "entities_enabled = True\n",
        "spacyvecs_enabled = True\n",
        "\n",
        "evaluate_only=True\n",
        "\n",
        "lstm_layers = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-HRVtRiWQVd"
      },
      "source": [
        "**If running from console this cell should not be executed.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56WWtjThWut_",
        "outputId": "5555a9bc-58a6-4303-db0f-5f03cdd51ec6"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# IF RUNNING FROM CONSOLE THE FOLLOWING CODE SHOULD NOT BE EXCUTED. \n",
        "\n",
        "\n",
        "argv = sys.argv[1:]\n",
        "\n",
        "try:\n",
        "    opts, args = getopt.getopt(argv,\"\",[\"seed=\",\n",
        "                                        \"model_name=\",\n",
        "                                        \"epochs=\",\n",
        "                                        \"bert_enabled=\",\n",
        "                                        'bert_sent_enabled=',\n",
        "                                        \"pos_enabled=\",\n",
        "                                        \"dep_enabled=\",\n",
        "                                        \"tag_enabled=\",\n",
        "                                        \"words_enabled=\",\n",
        "                                        \"entities_enabled=\",\n",
        "                                        \"lstm_layers=\",\n",
        "                                        'spacyvecs_enabled=',\n",
        "                                        'evaluate_only='                                        \n",
        "                                       ])\n",
        "except getopt.GetoptError:\n",
        "    print('Bad parameters')\n",
        "    sys.exit(2)\n",
        "for opt, arg in opts:\n",
        "    if opt == \"--seed\":\n",
        "        my_seed = int(arg)\n",
        "    elif opt == \"--model_name\":\n",
        "        model_name = arg\n",
        "    elif opt == \"--epochs\":\n",
        "        epochs = int(arg)\n",
        "    elif opt == \"--bert_enabled\":\n",
        "        bert_enabled = arg==\"True\"\n",
        "    elif opt == \"--pos_enabled\":\n",
        "        pos_enabled = arg==\"True\"\n",
        "    elif opt == \"--dep_enabled\":\n",
        "        dep_enabled = arg==\"True\"\n",
        "    elif opt == \"--tag_enabled\":\n",
        "        tag_enabled = arg==\"True\"\n",
        "    elif opt == \"--words_enabled\":\n",
        "        words_enabled = arg==\"True\"\n",
        "    elif opt == \"--entities_enabled\":\n",
        "        entities_enabled = arg==\"True\"\n",
        "    elif opt == \"--lstm_layers\":\n",
        "        lstm_layers = [int(value) for value in arg.split(',')]\n",
        "    elif opt == \"--spacyvecs_enabled\":\n",
        "        spacyvecs_enabled = arg==\"True\"\n",
        "    elif opt == '--bert_sent_enabled':\n",
        "        bert_sent_enabled = arg=='True'\n",
        "    elif opt == '--evaluate_only':\n",
        "        evaluate_only = arg=='True'\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bad parameters\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/work02/mariano/enviroments/venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgotJaRLWXBa"
      },
      "source": [
        "model_name = '{}_{}_({})'.format(model_name,'_'.join([str(value) for value in lstm_layers]), my_seed)\n",
        "                          \n",
        "print(\"Parameters:\")\n",
        "print(\"\\tseed: {}\".format(my_seed))\n",
        "print('\\tmodel_name: {}'.format(model_name))\n",
        "\n",
        "print('bert_enabled: {}'.format(bert_enabled))\n",
        "print('dep_enabled: {}'.format(dep_enabled))\n",
        "print('tag_enabled: {}'.format(tag_enabled))\n",
        "print('pos_enabled: {}'.format(pos_enabled))\n",
        "print('words_enabled: {}'.format(words_enabled))\n",
        "print('entities_enabled: {}'.format(entities_enabled))\n",
        "print('spacyvecs_enabled: {}'.format(spacyvecs_enabled))\n",
        "print('bert_sent_enabled: {}'.format(bert_sent_enabled))\n",
        "\n",
        "if evaluate_only:\n",
        "    print('[WARNING] EVALUATE ONLY !!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJwKT2ruWuuQ"
      },
      "source": [
        "### Paths "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJnliHssWuuT",
        "outputId": "84440645-53ef-4640-b0ca-b9352b7f35be"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(my_seed)\n",
        "\n",
        "# home_path = '/home/maiso/Event Detection/My RNN Model/'\n",
        "# data_path = '/home/maiso/Event Detection/data'\n",
        "\n",
        "\n",
        "# For saving the model information:\n",
        "home_path = '/home/mariano/work/python3.workspace/Event Detection - Experimentos Finales/My RNN Model/'\n",
        "\n",
        "# Where to look up for the event detection data set (it is not in the same format as in the webpage)\n",
        "data_path = '/home/mariano/work/python3.workspace/Event Detection - Experimentos Finales/data/'\n",
        "\n",
        "if not os.path.exists(home_path):\n",
        "    home_path = '/home/maiso/Event Detection/My RNN Model/'\n",
        "    data_path = '/home/maiso/Event Detection/data'\n",
        "word2vec_path = os.path.join(data_path,'word2vec/GoogleNews-vectors-negative300.bin')\n",
        "my_word2vec_path = os.path.join(data_path, 'word2vec/my_word2vec.p')\n",
        "\n",
        "training_sents_path = os.path.join(data_path,'training_sents.p') \n",
        "testing_sents_path = os.path.join(data_path,'testing_sents.p')\n",
        "\n",
        "\n",
        "os.makedirs(os.path.join(home_path,'models/{}'.format(model_name)), exist_ok=True)\n",
        "\n",
        "best_model_weights = os.path.join(home_path,'models/{}/weights.h5'.format(model_name))\n",
        "best_model_architecture = os.path.join(home_path,'models/{}/architecture.json'.format(model_name))\n",
        "history_path = os.path.join(home_path, 'models/{}/history.p'.format(model_name))\n",
        "\n",
        "\n",
        "\n",
        "flags = [bert_enabled, bert_sent_enabled, dep_enabled, tag_enabled, pos_enabled, words_enabled, entities_enabled,spacyvecs_enabled]\n",
        "flags"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[True, True, False, False, False, False, True, True]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_MSJmKoWuub"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1KFB1R5Wuuc",
        "outputId": "ce285942-7bc4-4288-b2b6-dae0f5ea470f"
      },
      "source": [
        "import pickle\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "oraciones = pickle.load(open(training_sents_path, 'rb'))\n",
        "for oracion in oraciones:\n",
        "    if 'spacy_doc' not in oracion:\n",
        "        oracion['spacy_doc'] = nlp(oracion['texto'])\n",
        "        \n",
        "oraciones_testing = pickle.load(open(testing_sents_path, 'rb'))\n",
        "for oracion in oraciones_testing:\n",
        "    if 'spacy_doc' not in oracion:\n",
        "        oracion['spacy_doc'] = nlp(oracion['texto'])\n",
        "print(len(oraciones))\n",
        "print(len(oraciones_testing))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxK09CNyWuuj"
      },
      "source": [
        "### Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQSbK9emWuuk"
      },
      "source": [
        "##### POStag && DEPtag && reduced POStag\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zeg3wSdWuum",
        "outputId": "8fd3b372-1bb7-46a3-8d31-948d89b6bd3a"
      },
      "source": [
        "tag_vocab = list(set([token.tag_ for oracion in oraciones+oraciones_testing for token in oracion['spacy_doc'] ]))\n",
        "tag2index = dict([(tag,idx) for idx,tag in enumerate(tag_vocab)])\n",
        "print('size of POSTag: {}'.format(len(tag2index)))\n",
        "\n",
        "dep_vocab = list(set([token.dep_ for oracion in oraciones+oraciones_testing for token in oracion['spacy_doc'] ]))\n",
        "dep2index = dict([(dep,idx) for idx,dep in enumerate(dep_vocab)])\n",
        "print('size of dependency tags: {}'.format(len(dep2index)))\n",
        "\n",
        "pos_vocab = list(set([token.pos_ for oracion in oraciones+oraciones_testing for token in oracion['spacy_doc'] ]))\n",
        "pos2index = dict([(pos,idx) for idx,pos in enumerate(pos_vocab)])\n",
        "print('size of pos tags: {}'.format(len(pos2index)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of POSTag: 47\n",
            "size of dependency tags: 47\n",
            "size of pos tags: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30chMCFZWuut"
      },
      "source": [
        "##### entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_HiFLZkWuuu",
        "outputId": "579bbca5-3749-4eec-ec23-5de0e5108677"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# # # # # # # # # # # # # # \n",
        "# GENERACIÓN ENTITY VOCAB #\n",
        "# # # # # # # # # # # # # # \n",
        "entity_vocab = set()\n",
        "for oracion in oraciones:\n",
        "    if not 'spacy_doc' in oracion:\n",
        "        oracion['spacy_doc'] = nlp(oracion['texto'])\n",
        "    assert all([token.idx==i and (token.idx+len(token.text))==f for (i,f),token in zip(oracion['tokens'], oracion['spacy_doc'])]), '{}!={}'.format(token.i,i)\n",
        "    e = [token.ent_iob_+'-'+token.ent_type_ for token in oracion['spacy_doc']]\n",
        "    entity_vocab.update(set(e))\n",
        "for oracion in oraciones_testing:\n",
        "    if not 'spacy_doc' in oracion:\n",
        "        oracion['spacy_doc'] = nlp(oracion['texto'])\n",
        "    assert all([token.idx==i and (token.idx+len(token.text))==f for (i,f),token in zip(oracion['tokens'], oracion['spacy_doc'])]), '{}!={}'.format(token.i,i)\n",
        "    e = [token.ent_iob_+'-'+token.ent_type_ for token in oracion['spacy_doc']]\n",
        "    entity_vocab.update(set(e))\n",
        "\n",
        "entity_vocab.add('[PAD]')\n",
        "entity_vocab = list(entity_vocab)\n",
        "ent2index = dict([(ent,index) for index,ent in enumerate(entity_vocab)])\n",
        "print('entity vocabulary size: {}'.format(len(ent2index)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "entity vocabulary size: 35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gusg6-NFWuu1"
      },
      "source": [
        "##### word2vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwmRMXJeWuu1",
        "outputId": "606f9f5e-5027-4943-d146-f16c18d1d3f1"
      },
      "source": [
        "import gensim\n",
        "# # # # # # # # # # # # \n",
        "# GENERACIÓN WORD2VEC #\n",
        "# # # # # # # # # # # # \n",
        "if not os.path.isfile(my_word2vec_path):\n",
        "    word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True) \n",
        "    # GENERACIÓN\n",
        "    my_word2vec = {}\n",
        "    for oracion in oraciones:\n",
        "        for ini,fin in oracion['tokens']:\n",
        "            token = oracion['texto'][ini:fin]\n",
        "            if token in word2vec_model:\n",
        "                my_word2vec[token] = word2vec_model[token]\n",
        "                \n",
        "    for oracion in oraciones_testing:\n",
        "        for ini,fin in oracion['tokens']:\n",
        "            token = oracion['texto'][ini:fin]\n",
        "            if token in word2vec_model:\n",
        "                my_word2vec[token] = word2vec_model[token]\n",
        "\n",
        "    pickle.dump(my_word2vec, open(my_word2vec_path, 'wb'))\n",
        "else:\n",
        "    my_word2vec = pickle.load(open(my_word2vec_path,'rb'))\n",
        "    \n",
        "vocab = set(my_word2vec.keys())\n",
        "vocab.update(set(['[UNK]']))\n",
        "vocab = list(vocab)\n",
        "print('vocabulary size: {}'.format(len(vocab)))\n",
        "word2index = dict([(word,idx) for idx, word in enumerate(vocab)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabulary size: 8646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ffziguyWuu6"
      },
      "source": [
        "##### spacy word2vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxB8o58bWuu7",
        "outputId": "dcb682f1-cf1f-48b1-deb1-dec43b5fedbf"
      },
      "source": [
        "spacy_vocab = set()\n",
        "for oracion in oraciones:\n",
        "    for token in oracion['spacy_doc']:\n",
        "        if token.has_vector:\n",
        "            spacy_vocab.add(token.text)\n",
        "for oracion in oraciones_testing:\n",
        "    for token in oracion['spacy_doc']:\n",
        "        if token.has_vector:\n",
        "            spacy_vocab.add(token.text)\n",
        "spacy_vocab.add('[UNK]')\n",
        "spacyword2index = dict([(word,idx) for idx, word in enumerate(spacy_vocab)])\n",
        "len(spacyword2index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9113"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHn8UiR2WuvA"
      },
      "source": [
        "### The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaWOzlI0WuvA"
      },
      "source": [
        "##### Custom metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVQP13k6WuvC",
        "outputId": "9ba26129-18c7-49fc-b631-fd85d076971e"
      },
      "source": [
        "from keras.callbacks import K\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    sens =  true_positives / (possible_positives + K.epsilon())\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
        "    spec = true_negatives / (possible_negatives + K.epsilon())\n",
        "    return 2*((sens*spec)/(sens+spec+K.epsilon()))\n",
        "\n",
        "def sensitivity(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "def specificity(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
        "    return true_negatives / (possible_negatives + K.epsilon())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/work02/mariano/enviroments/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/work02/mariano/enviroments/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/work02/mariano/enviroments/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/work02/mariano/enviroments/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/work02/mariano/enviroments/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/work02/mariano/enviroments/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iogSb9T5WuvH"
      },
      "source": [
        "##### callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4M688kSWuvI"
      },
      "source": [
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "mc = ModelCheckpoint(best_model_weights, monitor='val_f1_score', save_best_only=True,mode='max')\n",
        "es = EarlyStopping(monitor='val_f1_score', mode='max', verbose=1, patience=400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfFTTo9sWuvM"
      },
      "source": [
        "##### the architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXUqb4ndWuvN",
        "outputId": "77e8df60-533f-4e66-ea2e-b26f8f1774ba"
      },
      "source": [
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense, Embedding,Input, Dropout, Bidirectional, LSTM# Create the model\n",
        "import numpy as np\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Lambda\n",
        "import tensorflow as tf\n",
        "from keras.backend import expand_dims\n",
        "from keras.layers.merge import concatenate\n",
        "import tensorflow\n",
        "from keras.models import model_from_json\n",
        "# clear session\n",
        "from keras.regularizers import l1_l2\n",
        "\n",
        "# tensorflow.keras.backend.clear_session()\n",
        "# Parametros\n",
        "if evaluate_only:\n",
        "    # load json and create model\n",
        "    print('[WARNING] Model retrieved from JSON')\n",
        "    \n",
        "    json_file = open(best_model_architecture, 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    model = model_from_json(loaded_model_json)\n",
        "else:\n",
        "    embedding_size = 300\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "\n",
        "\n",
        "    # Modelo\n",
        "    bert_input = Input((None,768))\n",
        "    bert_sent_input = Input((None,768))\n",
        "    #embedding_wrd = Embedding(len(word2index),96, weights=[embedding_matrix])(input_wrd) #pretrainned weights\n",
        "\n",
        "    input_dep = Input((None,))\n",
        "    embedding_dep  = Embedding(len(dep2index), 10)(input_dep)\n",
        "\n",
        "    input_tag = Input((None,))\n",
        "    embedding_tag= Embedding(len(tag2index), 10)(input_tag)\n",
        "\n",
        "    input_pos = Input((None,))\n",
        "    embedding_pos  = Embedding(len(pos2index), 10)(input_pos)\n",
        "\n",
        "    input_word = Input((None,))\n",
        "\n",
        "    embedding_matrix = np.random.random((len(word2index), 300))\n",
        "    visitados = set()\n",
        "    for oracion in oraciones:\n",
        "        for token in oracion['spacy_doc']:\n",
        "            if not token.text in visitados and token.text in word2index:\n",
        "                embedding_matrix[word2index[token.text]]= my_word2vec[token.text]\n",
        "                visitados.add(token.text)\n",
        "\n",
        "    embedding_word  = Embedding(len(word2index), 300, weights=[embedding_matrix] )(input_word)\n",
        "\n",
        "    ## \n",
        "    input_spacyword = Input((None,))\n",
        "\n",
        "    embedding_matrix = np.random.random((len(spacyword2index), 96))\n",
        "    visitados = set()\n",
        "    for oracion in oraciones:\n",
        "        for token in oracion['spacy_doc']:\n",
        "            if not token.text in visitados and token.text in spacyword2index:\n",
        "                embedding_matrix[spacyword2index[token.text]]= token.vector\n",
        "                visitados.add(token.text)\n",
        "    for oracion in oraciones_testing:\n",
        "        for token in oracion['spacy_doc']:\n",
        "            if not token.text in visitados and token.text in spacyword2index:\n",
        "                embedding_matrix[spacyword2index[token.text]]= token.vector\n",
        "                visitados.add(token.text)\n",
        "\n",
        "    embedding_spacyword  = Embedding(len(spacyword2index), 96, weights=[embedding_matrix] )(input_spacyword)\n",
        "    ##\n",
        "\n",
        "    input_entities = Input((None,))\n",
        "    embedding_entities  = Embedding(len(ent2index), 10)(input_entities)\n",
        "\n",
        "    layers = [bert_input, \n",
        "                  bert_sent_input,\n",
        "                         embedding_dep,\n",
        "                         embedding_tag,\n",
        "                         embedding_pos,\n",
        "                         embedding_word,\n",
        "                         embedding_entities,\n",
        "                         embedding_spacyword\n",
        "                         ]\n",
        "\n",
        "    layers = [layer for layer,flag in zip(layers,flags) if flag]\n",
        "\n",
        "    if len(layers)>1:\n",
        "        merged = concatenate(layers)\n",
        "    else:\n",
        "        merged = layers[0]\n",
        "\n",
        "    # merged = concatenate([bert_input, \n",
        "    #                      embedding_dep,\n",
        "    #                      embedding_tag,\n",
        "    #                      embedding_pos,\n",
        "    #                      embedding_word,\n",
        "    #                      embedding_entities                     \n",
        "    #                      ])\n",
        "\n",
        "    # Arquitectura en conjunto\n",
        "    dropout_1 = Dropout(0.1)(merged)\n",
        "\n",
        "    last_layer = dropout_1\n",
        "    for lstm in lstm_layers:\n",
        "        lstm_i = Bidirectional(LSTM(lstm,return_sequences=True,activity_regularizer=l1_l2(0.001,0.001)))(last_layer)\n",
        "        last_layer = lstm_i\n",
        "    # lstm_1 = Bidirectional(LSTM(100,return_sequences=True,activity_regularizer=l1_l2(0.001,0.001)))(dropout_1)\n",
        "    # lstm_2 = Bidirectional(LSTM(15,return_sequences=True,activity_regularizer=l1_l2(0.001,0.001)))(lstm_1)\n",
        "    # lstm_3 = Bidirectional(LSTM(5,return_sequences=True,activity_regularizer=l1_l2(0.001,0.001)))(lstm_2)\n",
        "\n",
        "    dense_out = Dense(1, activation='sigmoid')(last_layer)\n",
        "\n",
        "    inputs = [bert_input, bert_sent_input, input_dep, input_tag, input_pos, input_word, input_entities,input_spacyword]\n",
        "\n",
        "    inputs = [input_ for input_,flag in zip(inputs,flags) if flag]\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=[dense_out])\n",
        "\n",
        "    model_json = model.to_json()\n",
        "    with open(best_model_architecture, \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "\n",
        "    \n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy',sensitivity,specificity, f1_score])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[WARNING] Model retrieved from JSON\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            (None, None, 768)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, None, 768)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, None, 10)     350         input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, None, 96)     874848      input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, 1642)   0           input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "                                                                 embedding_6[0][0]                \n",
            "                                                                 embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, None, 1642)   0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 1)      1643        dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 876,841\n",
            "Trainable params: 876,841\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz6gDoiHWuvU"
      },
      "source": [
        "##### the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB7AN5lqWuvV",
        "outputId": "5f3e7a55-a2cd-493f-94a7-9b7959c2fa90"
      },
      "source": [
        "import numpy as np\n",
        "def sent2xlist(oracion):\n",
        "#     xbert = np.zeros(shape=(1,len(oracion['spacy_doc']),768)) #oracion.bert_embeddings  # input layer, not embeddings\n",
        "    xbert = np.array(oracion['bert'])[np.newaxis,:,:]\n",
        "    xsentbert = np.average(xbert, axis=1) *np.ones(shape=(1,len(xbert[0,:,0]),768))\n",
        "    xdep = np.zeros(shape=(1,len(oracion['spacy_doc']))) # one-dimension porque son embeddings.\n",
        "    xtag = np.zeros(shape=(1,len(oracion['spacy_doc'])))\n",
        "    xpos = np.zeros(shape=(1,len(oracion['spacy_doc'])))\n",
        "    \n",
        "    xwrd = np.zeros(shape=(1,len(oracion['spacy_doc']))) # embedding\n",
        "    \n",
        "    xents = np.zeros(shape=(1,len(oracion['spacy_doc'])))\n",
        "    \n",
        "    xspacy = np.zeros(shape=(1,len(oracion['spacy_doc'])))\n",
        "    \n",
        "    \n",
        "#     for idx,token in enumerate(oracion.spacy_doc):\n",
        "#         xwrd[0,idx] = word2index[token.lemma_.lower()]\n",
        "    for idx,token in enumerate(oracion['spacy_doc']):\n",
        "        xdep[0,idx] = dep2index[token.dep_]\n",
        "    for idx,token in enumerate(oracion['spacy_doc']):\n",
        "        xtag[0,idx] = tag2index[token.tag_]\n",
        "    for idx,token in enumerate(oracion['spacy_doc']):\n",
        "        xpos[0,idx] = pos2index[token.pos_]\n",
        "        \n",
        "    for idx,token in enumerate(oracion['spacy_doc']):\n",
        "        if token.text in word2index:\n",
        "            xwrd[0,idx] = word2index[token.text]\n",
        "        else:\n",
        "            xwrd[0,idx] = word2index['[UNK]']\n",
        "        \n",
        "    for idx,token in enumerate(oracion['spacy_doc']):\n",
        "        xents[0,idx] = ent2index[token.ent_iob_+'-'+token.ent_type_]\n",
        "        \n",
        "    for idx,token in enumerate(oracion['spacy_doc']):\n",
        "        xspacy[0,idx] = spacyword2index[token.text]\n",
        "    \n",
        "    return [xbert, xsentbert, xdep, xtag, xpos,xwrd, xents, xspacy]\n",
        "sent2xlist(oraciones[0])\n",
        "\n",
        "def g(label_data):\n",
        "        j = 0\n",
        "        while True:\n",
        "            yield label_data[j]\n",
        "            j+=1\n",
        "            if j==len(label_data):\n",
        "                j=0\n",
        "                \n",
        "sent2xlist(oraciones[0])[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 33, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmxUAI5GWuvZ"
      },
      "source": [
        "##### the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QDcj53wTWuva",
        "outputId": "6a266603-455f-40ff-aad3-b296f9bdf16d"
      },
      "source": [
        "x = [sent2xlist(oracion) for oracion in oraciones]\n",
        "y = [np.array(oracion['etiquetas'])[np.newaxis,:,np.newaxis] for oracion in oraciones]\n",
        "\n",
        "x = [[elem for elem,flag in zip(list_,flags) if flag] for list_ in x]\n",
        "\n",
        "indices = np.arange(len(x))\n",
        "np.random.shuffle(indices)\n",
        "train_idx,val_idx = set(indices[:int(len(indices)*.80)]), set(indices[int(len(indices)*.8):])\n",
        "\n",
        "x_train, x_val = [elem for idx,elem in enumerate(x) if idx in train_idx], [elem for idx,elem in enumerate(x) if idx in val_idx]\n",
        "y_train, y_val = [elem for idx,elem in enumerate(y) if idx in train_idx], [elem for idx,elem in enumerate(y) if idx in val_idx]\n",
        "# x_train = [sent2xlist(oracion) for oracion in oraciones]\n",
        "# y_train = [np.array(oracion['etiquetas'])[np.newaxis,:,np.newaxis] for oracion in oraciones]\n",
        "\n",
        "# x_val = [sent2xlist(oracion) for oracion in oraciones]\n",
        "# y_val = [np.array(oracion['etiquetas'])[np.newaxis,:,np.newaxis] for oracion in oraciones]\n",
        "\n",
        "if not evaluate_only:\n",
        "    history = model.fit_generator(g([(x,y) for x,y in zip(x_train,y_train)]), \n",
        "                                   epochs=epochs, \n",
        "                                   steps_per_epoch=len(x_train), \n",
        "                                   callbacks=[es,mc],\n",
        "                                   validation_data=g([(x,y) for x,y in zip(x_val,y_val)]),\n",
        "                                   validation_steps=len(x_val), \n",
        "                                  verbose=2)\n",
        "\n",
        "    pickle.dump(history.history, open(history_path, 'wb'))\n",
        "    \n",
        "else:\n",
        "    print('[WARNING] NO TRAINING, EVALUATE ONLY')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[WARNING] NO TRAINING, EVALUATE ONLY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uhdA1WrWuve",
        "outputId": "5fcc9d05-9f00-450b-9a8f-34ae2073a08c"
      },
      "source": [
        "print('[  OK   ] Loading best weights from H5.')\n",
        "model.load_weights(best_model_weights)\n",
        "\n",
        "print('[  OK   ] Loading test set')\n",
        "x_test = [sent2xlist(oracion) for oracion in oraciones_testing]\n",
        "y_test = [np.array(oracion['etiquetas'])[np.newaxis,:,np.newaxis] for oracion in oraciones_testing]\n",
        "\n",
        "x_test = [[elem for elem,flag in zip(list_,flags) if flag] for list_ in x_test]\n",
        "\n",
        "print('[  OK   ] Performing the prediction.')\n",
        "\n",
        "print()\n",
        "print('[ INFO  ] TRAINING')\n",
        "values = model.evaluate_generator(g([(x,y) for x,y in zip(x_train,y_train)]), steps=len(x_train))\n",
        "\n",
        "for metric,value in zip(model.metrics_names, values):\n",
        "    print('{}: {:5.4f}'.format(metric, value))\n",
        "\n",
        "print()\n",
        "print('[ INFO  ] VAL')\n",
        "values = model.evaluate_generator(g([(x,y) for x,y in zip(x_val,y_val)]), steps=len(x_val))\n",
        "\n",
        "for metric,value in zip(model.metrics_names, values):\n",
        "    print('{}: {:5.4f}'.format(metric, value))\n",
        "\n",
        "\n",
        "print()\n",
        "print('[ INFO  ] TESTING')\n",
        "values = model.evaluate_generator(g([(x,y) for x,y in zip(x_test,y_test)]), steps=len(x_test))\n",
        "\n",
        "for metric,value in zip(model.metrics_names, values):\n",
        "    print('{}: {:4.4f}'.format(metric, value))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  OK   ] Loading best weights from H5.\n",
            "[  OK   ] Loading test set\n",
            "[  OK   ] Performing the prediction.\n",
            "\n",
            "[ INFO  ] TRAINING\n",
            "loss: 0.2090\n",
            "acc: 0.9403\n",
            "sensitivity: 0.2131\n",
            "specificity: 0.9966\n",
            "f1_score: 0.2691\n",
            "\n",
            "[ INFO  ] VAL\n",
            "loss: 0.2045\n",
            "acc: 0.9424\n",
            "sensitivity: 0.2232\n",
            "specificity: 0.9968\n",
            "f1_score: 0.2765\n",
            "\n",
            "[ INFO  ] TESTING\n",
            "loss: 0.1635\n",
            "acc: 0.9529\n",
            "sensitivity: 0.2328\n",
            "specificity: 0.9968\n",
            "f1_score: 0.2703\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}